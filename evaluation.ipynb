{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_directory = \"/home/wej36how/codes/CoSOD-main/result/Predictions/NWRDFRust\"\n",
    "groundTruths_directory = \"/home/wej36how/datasets/NWRDF/test/masks\"\n",
    "save_dir = main_directory+\"_concatenated\"\n",
    "patch_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copies all the subdirectories containing parts in their name to their corresponding main image direcotry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_subdir(main_dir):\n",
    "    subdirs = [d for d in main_dir.iterdir() if d.is_dir()]\n",
    "    for subdir in subdirs:\n",
    "        parts_dirs = main_dir.glob(f'{subdir.name}_*')\n",
    "        for part_dir in parts_dirs:\n",
    "            for file in part_dir.iterdir():\n",
    "                if file.is_file():\n",
    "                    shutil.move(str(file), str(subdir))\n",
    "            shutil.rmtree(part_dir)\n",
    "            \n",
    "organing_subdir(prediction_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate The patches to form the final prediction Map and store it in save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_position(img_shape, patch_size, patch_no):\n",
    "    num_patches_horizontally = img_shape[1] // patch_size[1]\n",
    "    num_patches_vertically = img_shape[0] // patch_size[0]\n",
    "    \n",
    "    if patch_no >= num_patches_horizontally * num_patches_vertically:\n",
    "        raise ValueError(\"Invalid patch number for the given image size and patch size.\")\n",
    "    \n",
    "    row = patch_no // num_patches_horizontally\n",
    "    col = patch_no % num_patches_horizontally\n",
    "    \n",
    "    return row, col\n",
    "\n",
    "def replace_with_patch(matrix, patch, row, col, patch_size):\n",
    "    start_y = row * patch_size[0]\n",
    "    start_x = col * patch_size[1]\n",
    "\n",
    "    end_y = start_y + patch_size[0]\n",
    "    end_x = start_x + patch_size[1]\n",
    "\n",
    "    end_y = min(end_y, matrix.shape[0])\n",
    "    end_x = min(end_x, matrix.shape[1])\n",
    "\n",
    "    patch = patch[:end_y - start_y, :end_x - start_x]\n",
    "    matrix[start_y:end_y, start_x:end_x] = patch\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def concatenate_patches():\n",
    "    masks = glob.glob(main_directory+'/*')\n",
    "    for mask in masks:\n",
    "        patches = glob.glob(f\"{mask}/*\")\n",
    "        imgNo = mask.split('/')[-1]\n",
    "        orig_mask_path = ground_truth_masks_directory+f\"/{imgNo}.png\"\n",
    "        img = cv2.imread(orig_mask_path, 0)\n",
    "        pred = np.zeros(img.shape)\n",
    "        for patch_path in patches:\n",
    "            patchNo = patch_path.split('/')[-1][:-4].split('.')[0].split('_')[-1]\n",
    "            row, col = get_patch_position(img.shape, patch_size, int(patchNo))\n",
    "            patch = cv2.imread(patch_path, 0)\n",
    "            patch = cv2.resize(patch, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "            pred = replace_with_patch(pred, patch, row, col, patch_size)\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        cv2.imwrite(os.path.join(save_dir, f\"{imgNo}.png\"), pred)\n",
    "\n",
    "concatenate_patches()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Precision, Recall, F1, IOU for rust class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(groundTruths_directory, predictions_directory):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    iou_scores = []\n",
    "\n",
    "    for filename in os.listdir(predictions_directory):\n",
    "        pred_path = os.path.join(predictions_directory, filename)\n",
    "        gt_path = os.path.join(groundTruths_directory, filename)\n",
    "       \n",
    "        if os.path.exists(pred_path) and os.path.exists(gt_path):\n",
    "            pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "            gt_img = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            pred_flat = pred_img.flatten()\n",
    "            gt_flat = gt_img.flatten()\n",
    "            \n",
    "            pred_flat = (pred_flat > 127).astype(np.uint8)\n",
    "            gt_flat = (gt_flat > 127).astype(np.uint8)\n",
    "            \n",
    "            # Default average argument = binary (Only report results for the class specified by pos_label) \n",
    "            # Default pos_label = 1 so metrics are calculated for rust class only\n",
    "            precision = precision_score(gt_flat, pred_flat)\n",
    "            recall = recall_score(gt_flat, pred_flat)\n",
    "            f1 = f1_score(gt_flat, pred_flat)\n",
    "            iou = jaccard_score(gt_flat, pred_flat)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1_score = np.mean(f1_scores)\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "\n",
    "    print(f'Average Precision: {avg_precision:.4f}')\n",
    "    print(f'Average Recall: {avg_recall:.4f}')\n",
    "    print(f'Average F1 Score: {avg_f1_score:.4f}')\n",
    "    print(f'Average iou Score: {avg_iou:.4f}')\n",
    "\n",
    "Evaluate(groundTruths_directory, predictions_directory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
